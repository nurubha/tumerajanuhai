import re
import numpy as np
import string
import pandas as pd 
import matplotlib as mpl
import matplotlib.pyplot as plt
from subprocess import check_output
from wordcloud import WordCloud, STOPWORDS
stopwords = set(STOPWORDS)
data ="""We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells."""
wordcloud = WordCloud(
 background_color='white',
 stopwords=stopwords,
 max_words=200,
 max_font_size=40,
 random_state=42
 ).generate(data)
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 24))
axes[0].imshow(wordcloud)
axes[0].axis('off')
axes[1].imshow(wordcloud)
axes[1].axis('off')
axes[2].imshow(wordcloud)
axes[2].axis('off')
fig.tight_layout()
# Clean Data
sentences = """We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells."""
#strip it remove the space from the words
# remove special characters
sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)
# remove 1 letter words
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip()
# lower all characters
sentences = sentences.lower()
#Vocabulary
words = sentences.split()
vocab = set(words)
vocab_size = len(vocab)
embed_dim = 10
context_size = 2
# Creating the Dictionaries
word_to_ix = {word: i for i, word in enumerate(vocab)}
ix_to_word = {i: word for i, word in enumerate(vocab)}
ix_to_word
# DataBags
# data - [(context), target]
data = []
for i in range(2, len(words) - 2):
 context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]
 target = words[i]
 data.append((context, target))
print(data[:5])
# Embeddings
embeddings = np.random.random_sample(( vocab_size, embed_dim))
embeddings
# Linear Model
def linear(m, theta):
    w = theta
    return m.dot(w)

def log_softmax(x):
    e_x = np.exp(x - np.max(x))
    return np.log(e_x / e_x.sum())

def NLLLoss(logs, targets):
    out = logs[range(len(targets)), targets]
    return -out.sum() / len(out)

def log_softmax_crossentropy_with_logits(logits, target):
    out = np.zeros_like(logits)
    out[np.arange(len(logits)), target] = 1
    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)
    return (-out + softmax) / logits.shape[0]

def forward(context_idxs, theta):
    m = embeddings[context_idxs].reshape(1, -1)  # Get word embeddings and reshape
    n = linear(m, theta)  # Linear transformation
    o = log_softmax(n)    # Softmax over the linear output
    return m, n, o

def backward(preds, theta, target_idxs):
    m, n, o = preds
    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)  # Gradient of loss w.r.t. logits
    dw = m.T.dot(dlog)  # Backpropagate through the linear layer
    return dw
def optimize(theta, grad, lr=0.03):
    theta -= grad * lr  # Update the parameters using gradient descent
    return theta

# Training
theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))
epoch_losses = {}

# Train for 80 epochs
for epoch in range(80):
    losses = []
    for context, target in data:
        context_idxs = np.array([word_to_ix[w] for w in context])  # Convert context words to indices
        preds = forward(context_idxs, theta)  # Forward pass
        target_idxs = np.array([word_to_ix[target]])  # Convert target word to index
        loss = NLLLoss(preds[-1], target_idxs)  # Calculate loss
        losses.append(loss)
        grad = backward(preds, theta, target_idxs)  # Backpropagation
        theta = optimize(theta, grad, lr=0.03)  # Gradient descent

    epoch_losses[epoch] = losses  # Store losses per epoch

# Plotting the Loss
ix = np.arange(0, 80)
fig = plt.figure()
fig.suptitle('Epoch/Losses', fontsize=20)
plt.plot(ix, [epoch_losses[i][0] for i in ix])  # Plot loss for each epoch
plt.xlabel('Epochs', fontsize=12)
plt.ylabel('Losses', fontsize=12)
plt.show()
# Predict function
def predict(words):
    context_idxs = np.array([word_to_ix[w] for w in words])
    preds = forward(context_idxs, theta)
    word = ix_to_word[np.argmax(preds[-1])]
    return word

# Testing prediction
print(predict(['we', 'are', 'to', 'study']))  # Should predict 'about'

# Accuracy function
def accuracy():
    wrong = 0
    for context, target in data:
        if predict(context) != target:
            wrong += 1
    return (1 - (wrong / len(data)))

# Calculate accuracy
print(f"Accuracy: {accuracy() * 100:.2f}%")